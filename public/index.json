[
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Full Name: Nguyễn Thị Thu Hoài\nPhone Number: 0385208694\nEmail: hoainttd@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1217\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nWorklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "New general-purpose Amazon EC2 M8a instances are now available Today, we’re announcing the availability of Amazon Elastic Compute Cloud (Amazon EC2) M8a instances, the latest addition to the general-purpose M instance family. These instances are powered by the 5th Generation AMD EPYC (codename Turin) processors with a maximum frequency of 4.5GHz. Customers can expect up to 30% higher performance and up to 19% better price performance compared to M7a instances. They also provide higher memory bandwidth, improved networking and storage throughput, and flexible configuration options for a broad set of general-purpose workloads.\nImprovements in M8a\nM8a instances deliver up to 30% better performance per vCPU compared to M7a instances, making them ideal for applications that require benefit from high performance and high throughput such as financial applications, gaming, rendering, application servers, simulation modeling, midsize data stores, application development environments, and caching fleets.\nThey provide 45% more memory bandwidth compared to M7a instances, accelerating in-memory databases, distributed caches, and real-time analytics.\nFor workloads with high I/O requirements, M8a instances provide up to 75 Gbps of networking bandwidth and 60 Gbps of Amazon Elastic Block Store (Amazon EBS) bandwidth, a 50% improvement over the previous generation. These enhancements support modern applications that rely on rapid data transfer and low-latency network communication.\nEach vCPU on an M8a instance corresponds to a physical CPU core, meaning there is no simultaneous multithreading (SMT). In application benchmarks, M8a instances delivered up to 60% faster performance for GroovyJVM and up to 39% faster performance for Cassandra compared to M7a instances.\nM8a instances support instance bandwidth configuration (IBC), which provides flexibility to allocate resources between networking and EBS bandwidth. This gives customers the flexibility to scale network or EBS bandwidth by up to 25% and improve database performance, query processing, and logging speeds.\nM8a is available in ten virtualized sizes and two bare metal options (metal-24xl and metal-48xl), providing deployment choices that scale from small applications to large enterprise workloads. All of these improvements are built on the AWS Nitro System, which delivers low virtualization overhead, consistent performance, and advanced security across all instance sizes. These instances are built using the latest sixth generation AWS Nitro Cards, which offload and accelerate I/O for functions, increasing overall system performance.\nM8a instances feature sizes of up to 192 vCPU with 768GiB RAM. Here are the detailed specs:\nInstance size vCPUs Memory (GiB) Network bandwidth (Gbps) EBS bandwidth (Gbps) medium 1 4 Up to 12.5 Up to 10 large 2 8 Up to 12.5 Up to 10 xlarge 4 16 Up to 12.5 Up to 10 2xlarge 8 32 Up to 15 Up to 10 4xlarge 16 64 Up to 15 Up to 10 8xlarge 32 128 15 10 12xlarge 48 192 22.5 15 16xlarge 64 256 30 20 24xlarge 96 384 40 30 48xlarge 192 768 75 60 metal-24xl 96 384 40 30 metal-48xl 192 768 75 60 For a complete list of instance sizes and specifications, refer to the Amazon EC2 M8a instances page.\nWhen to use M8a instances M8a is a strong fit for general-purpose applications that need a balance of compute, memory, and networking. M8a instances are ideal for web and application hosting, microservices architectures, and databases where predictable performance and efficient scaling are important.\nThese instances are SAP certified and also well suited for enterprise workloads such as financial applications and enterprise resource planning (ERP) systems. They’re equally effective for in-memory caching and customer relationship management (CRM), in addition to development and test environments that require cost efficiency and flexibility. With this versatility, M8a supports a wide spectrum of workloads while helping customers improve price performance.\nNow available Amazon EC2 M8a instances are available today in US East (Ohio) US West (Oregon) and Europe (Spain) AWS Regions. M8a instances can be purchased as On-Demand, Savings Plans, and Spot Instances. M8a instances are also available on Dedicated Hosts. To learn more, visit the Amazon EC2 Pricing page.\nTo learn more, visit the Amazon EC2 M8a instances page and send feedback to AWS re:Post for EC2 or through your usual AWS support contacts.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "AWS FCJ Workforce Kick-off Event - FPTU OJT FALL 2025 Event Objectives Create a starting point for the AWS Builders journey\nIntroduce the AWS First Cloud Journey Workforce program\nConnect with experts and expand career opportunities\nList of Speakers Mr. Nguyen Tran Phuoc Bao - Head of Corporate Relations Office, delivering the opening speech\nNguyen Gia Hung - Head of Solutions Architect, AWS Vietnam: AWS First Cloud Journey \u0026amp; Future Direction\nDo Huy Thang - DevOps Lead, VNG: DevOps \u0026amp; Future Career\nDanh Hoang Hieu Nghi – GenAI Engineer, Renova: From First Cloud Journey to GenAI Engineer\nBui Ho Linh Nhi – AI Engineer, SoftwareOne: She in Tech \u0026amp; Journey with First Cloud Journey\nPham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific: A Day as a Cloud Engineer\nNguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific: Journey to the First Cloud Journey\nFeatured Content Meet AWS experts and partner companies\nPractical sharing about Cloud, AI, DevOps\nNetworking \u0026amp; career orientation\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;GenAI-powered App-DB Modernization workshop\u0026rdquo; Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.1-workshop-overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Goals In this lab, we will:\nLaunch an EC2 (Ubuntu) virtual server. Configure the network environment (Security Group) to allow web access. SSH connection and install Python/Streamlit environment. Deploy source code from Github and run the prediction app. Workflow End-users will access the EC2\u0026rsquo;s Public IP address via a web browser on port 8501. Inside the EC2, the Streamlit application will process requests, invoke the model, and return visualized results.\nArchitecture Diagram "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.1-week1/",
	"title": "Week 1",
	"tags": [],
	"description": "",
	"content": "Week 1 Goals (from 08/09/2025 - 12/09/2025) Understand cloud computing, global infrastructure, and basic AWS services. Understand AWS management tools, how to use the AWS Console and CLI. Learn how to optimize costs on AWS and how to work with AWS Support. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about cloud computing, global infrastructure, and AWS basic services - Practice creating your first AWS account https://000001.awsstudygroup.com/ Tuesday - Set up Multi-Factor Authentication for AWS Root User - Create Admin Group and Admin User; create and revoke Access Key for Root User - Explore AWS Management Console configuration: + Set default region + Search AWS services, features, and documentation + Add and remove favorite AWS services + Create and use console widgets https://000001.awsstudygroup.com/ Wednesday - Learn about AWS cost-optimization methods - Practice cost management with AWS Budgets + Create Cost Budget + Create Usage Budget + Create Reservation Budget + Create Savings Plans Budget https://000007.awsstudygroup.com/ Thursday - Learn about AWS Support with its four main support plans: Basic, Developer, Business, and Enterprise - Review how to upgrade support plans when critical issues require quick resolution https://000009.awsstudygroup.com/ Friday - Research the AWS Well-Architected Framework Week 1 Achievements: Understand what AWS is and grasp the basic service categories. Successfully created and configured an AWS account. Became familiar with the AWS Management Console and learned how to find, access, and use services from the web interface. "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "This section documents my 12-week internship journey at ITea Lab, covering AWS services learning, hands-on workshops, and final project development.\nWeek 1: Getting started with AWS and basic AWS services Week 2: Networking services on AWS Week 3: Compute VM services on AWS Week 4: Storage services on AWS Week 5: Security services on AWS Week 6: Security services on AWS Week 7: Database services on AWS Week 8: Data analysis on AWS Week 9: Data analysis on AWS Week 10: Preparing the Workshop topic Week 11: Implementing Workshop content Week 12: Finalizing Workshop and internship report "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Introducing new compute-optimized Amazon EC2 C8i and C8i-flex instances After launching Amazon Elastic Compute Cloud (Amazon EC2) memory-optimized R8i and R8i-flex instances and general-purpose M8i and M8i-flex instances, I am happy to announce the general availability of compute-optimized C8i and C8i-flex instances powered by custom Intel Xeon 6 processors available only on AWS with sustained all-core 3.9 GHz turbo frequency and feature a 2:1 ratio of memory to vCPU. These instances deliver the highest performance and fastest memory bandwidth among comparable Intel processors in the cloud.\nThe C8i and C8i-flex instances offer up to 15 percent better price-performance, and 2.5 times more memory bandwidth compared to C7i and C7i-flex instances. The C8i and C8i-flex instances are up to 60 percent faster for NGINX web applications, up to 40 percent faster for AI deep learning recommendation models, and 35 percent faster for Memcached stores compared to C7i and C7i-flex instances.\nC8i and C8i-flex instances are ideal for running compute-intensive workloads, such as web servers, caching, Apache.Kafka, ElasticSearch, batch processing, distributed analytics, high performance computing (HPC), ad serving, highly scalable multiplayer gaming, and video encoding.\nAs like other 8th generation instances, these instances use the new sixth generation AWS Nitro Cards, delivering up to two times more network and Amazon Elastic Block Storage (Amazon EBS) bandwidth compared to the previous generation instances. They also support bandwidth configuration with 25 percent allocation adjustments between network and Amazon EBS bandwidth, enabling better database performance, query processing, and logging speeds.\nC8i instances\nC8i instances provide up to 384 vCPUs and 768 GiB memory including bare metal instances that provide dedicated access to the underlying physical hardware. These instances help you to run compute-intensive workloads, such as CPU-based inference, and video streaming that need the largest instance sizes or high CPU continuously.\nHere are the specs for C8i instances:\nInstance Size vCPUs Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps) c8i.large 2 4 Up to 12.5 Up to 10 c8i.xlarge 4 8 Up to 12.5 Up to 10 c8i.2xlarge 8 16 Up to 15 Up to 10 c8i.4xlarge 16 32 Up to 15 Up to 10 c8i.8xlarge 32 64 15 10 c8i.12xlarge 48 96 22.5 15 c8i.16xlarge 64 128 30 20 c8i.24xlarge 96 192 40 30 c8i.32xlarge 128 256 50 40 c8i.48xlarge 192 384 75 60 c8i.96xlarge 384 768 100 80 c8i.metal-48xl 192 384 75 60 c8i.metal-96xl 384 768 100 80 C8i-flex instances\nC8i-flex instances are a lower-cost variant of the C8i instances, with 5 percent better price performance at 5 percent lower prices. These instances are designed for workloads that benefit from the latest generation performance but don’t fully utilize all compute resources. These instances can reach up to the full CPU performance 95 percent of the time.\nHere are the specs for the C8i-flex instances:\nInstance Size vCPUs Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps) c8i-flex.large 2 4 Up to 12.5 Up to 10 c8i-flex.xlarge 4 8 Up to 12.5 Up to 10 c8i-flex.2xlarge 8 16 Up to 15 Up to 10 c8i-flex.4xlarge 16 32 Up to 15 Up to 10 c8i-flex.8xlarge 32 64 Up to 15 Up to 10 c8i-flex.12xlarge 48 96 Up to 22.5 Up to 15 c8i-flex.16xlarge 64 128 Up to 30 Up to 20 If you’re currently using earlier generations of compute-optimized instances, you can adopt C8i-flex instances without having to make changes to your application or your workload.\nNow available\nAmazon EC2 C8i and C8i-flex instances are available today in the US East (N. Virginia), US East (Ohio), US West (Oregon), and Europe (Spain) AWS Regions. C8i and C8i-flex instances can be purchased as On-Demand, Savings Plan, and Spot instances. C8i instances are also available in Dedicated Instances and Dedicated Hosts. To learn more, visit the Amazon EC2 Pricing page.\nGive C8i and C8i-flex instances a try in the Amazon EC2 console. To learn more, visit the Amazon EC2 C8i instances page and send feedback to AWS re:Post for EC2 or through your usual AWS Support contacts.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "1. Create Key Pair You need a Key Pair (.pem format) to SSH into the EC2 instance.\nNavigate to EC2 Console \u0026gt; Key Pairs. Click Create key pair. Name: workshop-aws-streamlit. File format: .pem. 2. Configure Security Group To allow the application to be accessible from the internet, you must configure the Security Group (SG):\nGo to Security Groups \u0026gt; Create security group. Add the following Inbound rules: SSH (TCP 22): Source My IP (For administration). Custom TCP (TCP 8501): Source Anywhere-IPv4 (0.0.0.0/0) (For Streamlit web access). "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Customer Churn Prediction Platform on AWS A Machine Learning Deployment for Predicting Bank Customer Attrition 1. Executive Summary The Customer Churn Prediction Platform is an AWS-based solution designed to deploy and serve a Machine Learning model that predicts the likelihood of bank customers leaving (churning). The platform provides an internal web interface where analysts and bank staff can input customer information and instantly obtain churn probabilities.\nThe solution leverages Amazon EC2 to host a Streamlit web application, Amazon S3 to store the ML model and related data, AWS Identity and Access Management (IAM) for secure access, Amazon CloudWatch for monitoring, and Amazon VPC with Security Groups to control network traffic. The system is intentionally simple, cost-effective, and well suited for workshops, student projects, and small-scale production use within a bank’s analytics team.\n2. Problem Statement What’s the Problem? Today, the ML churn model is usually run on local notebooks or personal machines. Business users cannot directly access the model, and results must be generated manually by data scientists. This leads to:\nNo centralized, always-on environment for making predictions. Inefficient collaboration between data teams and business teams. Model files (.pkl / .joblib) being shared over email or chat, which is insecure and hard to manage. Difficulties in demonstrating the solution to stakeholders in a professional and repeatable way. The Solution The workshop proposes deploying a churn prediction model on AWS using a simple yet realistic architecture:\nA Streamlit web application running on Amazon EC2 provides the user interface and handles input/output. The trained ML model is stored in Amazon S3 and loaded into EC2 at startup for low-latency inference. AWS IAM Roles grant the EC2 instance secure access to the S3 bucket without hard-coding credentials. Amazon CloudWatch collects logs and metrics from the EC2 instance for observability. Amazon VPC and Security Groups restrict traffic to HTTPS/HTTP or port 8501 for Streamlit. This design turns an offline model into an online prediction service that can be accessed from any browser within the allowed network.\nBenefits and Return on Investment The platform enables bank teams to:\nRun churn predictions in real time to prioritize retention campaigns. Reduce manual work by ~80–90% compared to running notebooks on demand. Demonstrate an end-to-end ML deployment on AWS in a way that is easy to understand and extend. Estimated monthly cost for a small instance (e.g., t2.micro or t3.small) and light data usage is around 8–12 USD/month, which is highly affordable compared to the value provided in terms of faster decisions, better collaboration, and a reusable ML deployment blueprint.\n3. Solution Architecture The solution uses a straightforward AWS architecture for serving the churn model:\nUser (Web Browser) → accesses the Streamlit app via the EC2 public endpoint. Amazon VPC + Security Group → controls inbound traffic (e.g., HTTP/HTTPS or port 8501). Amazon EC2 → hosts the Streamlit application and loads the ML model into memory. Amazon S3 → stores the trained churn model file and optionally input/output logs. AWS IAM Role → grants EC2 permission to read the model from S3. Amazon CloudWatch → receives logs and metrics from EC2 for monitoring. AWS Services Used Amazon EC2: Hosts the Streamlit application and runs the ML inference code. Amazon S3: Stores the serialized ML model (.pkl/.joblib) and optional datasets or prediction logs. AWS IAM: Provides an instance role so EC2 can securely read from S3 without static keys. Amazon CloudWatch: Collects logs, CPU metrics, and alarms for the EC2 instance. Amazon VPC \u0026amp; Security Groups: Restrict incoming and outgoing traffic to the application. (Optional) Amazon API Gateway + AWS Lambda: Can be added later to expose the model as a REST API instead of (or in addition to) Streamlit. Component Design Web Frontend (Streamlit on EC2):\nRenders HTML forms for entering customer features (age, tenure, balance, products, etc.). Sends the input data to the backend logic in the same Streamlit app for prediction. Model Inference Layer:\nLoads the saved churn model from S3 (or from disk after first download). Performs data preprocessing and predicts churn probability. Returns results to the UI with human-readable labels (e.g., “High churn risk”). Storage and Access Control:\nS3 bucket holds the model file and optionally CSV/Parquet logs of predictions. IAM Role controls which EC2 instance can access which S3 resources. Monitoring and Operations:\nCloudWatch Logs captures application logs (errors, prediction counts). CloudWatch Metrics monitors CPU, memory (via CloudWatch agent), and network traffic. 4. Technical Implementation Implementation Phases\nThe workshop is divided into several phases, guiding participants from a local model to a deployed AWS solution:\nModel Preparation and Packaging\nTrain a churn prediction model locally or on a notebook. Export the model to a .pkl or .joblib file. Upload the model file to an S3 bucket. Infrastructure Setup on AWS\nLaunch an Amazon EC2 instance (Ubuntu / Amazon Linux). Configure the instance within a VPC and set up an appropriate Security Group. Attach an IAM Role that allows s3:GetObject on the model bucket. Application Development and Deployment\nInstall Python, pip, and required libraries (e.g., streamlit, pandas, scikit-learn). Clone the workshop GitHub repository to EC2. Add code in the Streamlit app to download and load the model from S3 at startup. Expose the application on port 8501 (or 80/443 with a reverse proxy). Testing and Hardening\nVerify that predictions are correct and consistent with the local notebook. Test access from a browser using the EC2 public DNS or IP. Configure CloudWatch for logs and alerts. 5. Timeline \u0026amp; Milestones Project Timeline (Workshop-Oriented)\nSession 1 – Introduction \u0026amp; Architecture (Week 1)\nOverview of churn prediction and business use cases. Walkthrough of the AWS architecture and services used. Session 2 – Model \u0026amp; S3 (Week 2)\nTrain/prepare the churn model. Upload the model to S3 and test access. Session 3 – EC2 \u0026amp; Streamlit (Week 3)\nLaunch EC2, configure security, install dependencies. Deploy the Streamlit app and connect it to the model on S3. Session 4 – Monitoring \u0026amp; Final Demo (Week 4)\nEnable CloudWatch logging and metrics. Run a full end-to-end demo and discuss improvements (e.g., API Gateway, CI/CD). 6. Budget Estimation The following cost estimation assumes low traffic and workshop/demo usage:\nEC2 t2.micro / t3.small: ~7–10 USD/month (on-demand). Amazon S3: ~0.05–0.20 USD/month (model file + small datasets). Data Transfer: ~0.02 USD/month (light inbound/outbound usage). CloudWatch Logs \u0026amp; Metrics: ~0.03 USD/month (basic logging). Estimated Total: ~8–12 USD/month for a single small environment.\nParticipants can be guided to use the AWS Pricing Calculator to recompute these values for their own scenarios.\n7. Risk Assessment Risk Matrix EC2 Downtime: Medium impact, low probability (can be mitigated with proper instance type and health checks). High CPU/Memory Usage during peak predictions: High impact, medium probability. Misconfiguration of IAM or Security Groups: Medium impact, low probability. Mitigation Strategies Use CloudWatch alarms to detect instance issues early. Scale up to a larger EC2 instance if CPU or memory usage becomes a bottleneck. Follow the principle of least privilege when defining IAM roles. Contingency Plans Keep an AMI (snapshot) or infrastructure-as-code template to quickly recreate the environment. In case of major failure, temporarily fall back to local notebook predictions while the AWS environment is restored. 8. Expected Outcomes Technical Improvements A working, browser-accessible churn prediction application deployed on AWS. Participants gain hands-on experience with EC2, S3, IAM, CloudWatch, and basic ML deployment practices. Long-term Value A reusable reference architecture for future ML projects (credit scoring, fraud detection, recommendation, etc.). A foundation that can be extended with CI/CD, API Gateway + Lambda, or containerization (ECS/EKS) in more advanced workshops. "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.2-week2/",
	"title": "Week 2",
	"tags": [],
	"description": "",
	"content": "Week 2 Goals (from 15/09/2025 - 19/09/2025) Clearly understand networking services on AWS. Become proficient in setting up and configuring VPC, VPC Peering, Transit Gateway, and Hybrid DNS. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about networking services on AWS - Learn about Amazon Virtual Private Cloud (VPC) + Subnet + Route Table + Elastic Network Interface + Elastic IP Address + VPC Endpoint + Internet Gateway + NAT Gateway - Learn theoretical concepts of VPN \u0026amp; Direct Connect + VPN Site-to-Site + AWS Direct Connect - Study theory about Elastic Load Balancing https://youtu.be/O9Ac_vGHquM?si=oQBDke9m6FJXdrhc https://youtu.be/BPuD1l2hEQ4?si=XHzAkIC6HG-mFRBj https://youtu.be/CXU8D3kyxIc?si=B08A98ymh5vYXJ7a Tuesday - Learn about VPC security and multi-VPC features + Security Group (SG) + Network Access Control List (NACL) + VPC Flow Logs + VPC Peering features + Transit Gateway - Practice creating a VPC and configuring Site-to-Site VPC https://000003.awsstudygroup.com/ Wednesday - Set up VPC Peering - Update Network ACL - Create Peering Connection - Configure Route Tables - Enable Cross-Peer DNS https://000019.awsstudygroup.com/ Thursday - Set up Transit Gateway - Preparation: initialize CloudFormation, create security groups, launch EC2 instances, and set up infrastructure - Create Transit Gateway - Create Transit Gateway Attachments - Create Transit Gateway Route Table, add Transit Gateway to VPC Route Table, and verify results https://000020.awsstudygroup.com/ Friday - Learn about Amazon Route 53 service - Set up Hybrid DNS + Create Route 53 Outbound Endpoint + Route 53 Resolver Rules + Route 53 Inbound Endpoints https://000010.awsstudygroup.com/ "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "AWS Transfer Family SFTP connectors now support VPC-based connectivity Many organizations rely on the Secure File Transfer Protocol (SFTP) as the industry standard for exchanging critical business data. Traditionally, securely connecting to private SFTP servers required custom infrastructure, manual scripting, or exposing endpoints to the public internet.\nToday, AWS Transfer Family SFTP connectors now support connectivity to remote SFTP servers through Amazon Virtual Private Cloud (Amazon VPC) environments. You can transfer files between Amazon Simple Storage Service (Amazon S3) and private or public SFTP servers while applying the security controls and network configurations already defined in your VPC. This capability helps you integrate data sources across on-premises environments, partner-hosted private servers, or internet-facing endpoints, with the operational simplicity of a fully managed Amazon Web Services (AWS) service.\nNew capabilities with SFTP connectors The following are the key enhancements:\nConnect to private SFTP servers – SFTP connectors can now reach endpoints that are only accessible within your AWS VPC connection. These include servers hosted in your VPC or a shared VPC, on-premises systems connected over AWS Direct Connect, and partner-hosted servers connected through VPN tunnels. Security and compliance – All file transfers are routed through the security controls already applied in your VPC, such as AWS Network Firewall or centralized ingress and egress inspection. Private SFTP servers remain private and don’t need to be exposed to the internet. You can also present static Elastic IP or bring your own IP (BYOIP) addresses to meet partner allowlist requirements. Performance and simplicity – By using your own network resources such as NAT gateways, AWS Direct Connect or VPN connections, connectors can take advantage of higher bandwidth capacity for large-scale transfers. You can configure connectors in minutes through the AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs without building custom scripts or third-party tools. How VPC- based SFTP connections work SFTP connectors use Amazon VPC Lattice resources to establish secure connectivity through your VPC. Key constructs include a resource configuration and a resource gateway. The resource configuration represents the target SFTP server, which you specify using a private IP address or public DNS name. The resource gateway provides SFTP connector access to these configurations, enabling file transfers to flow through your VPC and its security controls.\nThe following architecture diagram illustrates how traffic flows between Amazon S3 and remote SFTP servers. As shown in the architecture, traffic flows from Amazon S3 through the SFTP connector into your VPC. A resource gateway is the entry point that handles inbound connections from the connector to your VPC resources. Outbound traffic is routed through your configured egress path, using Amazon VPC NAT gateways with Elastic IPs for public servers or AWS Direct Connect and VPN connections for private servers. You can use existing IP addresses from your VPC CIDR range, simplifying partner server allowlists. Centralized firewalls in the VPC enforce security policies, and customer-owned NAT gateways provide higher bandwidth for large-scale transfers.\nWhen to use this feature With this capability, developers and IT administrators can simplify workflows while meeting security and compliance requirements across a range of scenarios:\nHybrid environments – Transfer files between Amazon S3 and on-premises SFTP servers using AWS Direct Connect or AWS Site-to-Site VPN, without exposing endpoints to the internet. Partner integrations – Connect with business partners’ SFTP servers that are only accessible through private VPN tunnels or shared VPCs. This avoids building custom scripts or managing third-party tools, reducing operational complexity. Regulated industries – Route file transfers through centralized firewalls and inspection points in VPCs to comply with financial services, government, or healthcare security requirements. High-throughput transfers – Use your own network configurations such as NAT gateways, AWS Direct Connect, or VPN connections with Elastic IP or BYOIP to handle large-scale, high-bandwidth transfers while retaining IP addresses already on partner allowlists. Unified file transfer solution – Standardize on Transfer Family for both internal and external SFTP connectivity, reducing fragmentation across file transfer tools. Start building with SFTP connectors To begin transferring files with SFTP connectors through my VPC environment, I follow these steps:\nFirst, I configure my VPC Lattice resources. In the Amazon VPC console, under PrivateLink and Lattice in the navigation pane**,** I choose Resource gateways, choose Create resource gateway to create one to act as the ingress point into my VPC.Next, under PrivateLink and Lattice in the navigation pane, I choose Resource configuration and choose Create resource configuration to create a resource configuration for my target SFTP server. Specify the private IP address or public DNS name, and the port (typically 22).\nThen, I configure AWS Identity and Access Management (IAM) permissions. I ensure that the IAM role used for connector creation has transfer:* permissions, and VPC Lattice permissions (vpc-lattice:CreateServiceNetworkResourceAssociation, vpc-lattice:GetResourceConfiguration, vpc-lattice:AssociateViaAWSService). I update the trust policy on the IAM role to specify transfer.amazonaws.com as a trusted principal. This enables AWS Transfer Family to assume the role when creating and managing my SFTP connectors.\nAfter that, I create an SFTP connector through the AWS Transfer Family console. I choose SFTP Connectors and then choose Create SFTP connector.In the Connector configuration section, I select VPC Lattice as the egress type, then provide the Amazon Resource Name (ARN) of the Resource Configuration, Access role, and Connector credentials. Optionally, include a trusted host key for enhanced security, or override the default port if my SFTP server uses a nonstandard port.\nNext, I test the connection. On the Actions menu, I choose Test connection to confirm that the connector can reach the target SFTP server.\nFinally, after the connector status is ACTIVE, I can begin file operations with my remote SFTP server programmatically by calling Transfer Family APIs such as StartDirectoryListing, StartFileTransfer, StartRemoteDelete, or StartRemoteMove. All traffic is routed through my VPC using my configured resources such as NAT gateways, AWS Direct Connect, or VPN connections together with my IP addresses and security controls.\nFor the complete set of options and advanced workflows, refer to the AWS Transfer Family documentation.\nNow available\nSFTP connectors with VPC-based connectivity are now available in 21 AWS Regions. Check the AWS Services by Region for the latest supported AWS Regions. You can now securely connect AWS Transfer Family SFTP connectors to private, on-premises, or internet-facing servers using your own VPC resources such as NAT gateways, Elastic IPs, and network firewalls.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.3-launch-ec2/",
	"title": "Launch EC2 Instance",
	"tags": [],
	"description": "",
	"content": "In this section, we will launch an EC2 virtual server to host the application.\nNavigate to EC2 Dashboard and click Launch Instance. Name: workshop-aws-streamlit. OS Images: Choose Ubuntu (Ubuntu Server 20.04 or 22.04 LTS). Instance Type: Choose t3.micro (or t2.micro for Free Tier). Key Pair: Select workshop-aws-streamlit created earlier. Network settings: Choose Select existing security group. Select the Security Group configured in section 5.2. Click Launch instance. Once launched, copy the Public IPv4 address of the instance for the next steps.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - New general-purpose Amazon EC2 M8a instances are now available Blog 2 - Introducing new compute-optimized Amazon EC2 C8i and C8i-flex instances Blog 3 - AWS Transfer Family SFTP connectors now support VPC-based connectivity Blog 4 - Simplifying sustainability reporting using AWS and generative AI in banking Blog 5 - The future of customer service is here: From contact centers to experience hubs Blog 6 - Optimize HPC Workloads with the Updated AWS Well-Architected Lens "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.3-week3/",
	"title": "Week 3",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals (from 22/09/2025 - 26/09/2025) Clearly understand Compute VM services on AWS. Become proficient in deploying, managing, and scaling EC2 virtual machines on AWS. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about Compute VM services on AWS + Amazon Elastic Compute Cloud (EC2): IAM, Backup, Key Pair, Elastic Block Store (EBS), Instance Store, User Data, Metadata, EC2 Auto Scaling, review EC2 pricing options + Amazon Lightsail + Amazon EFS/FSx + AWS Application Migration Service (MGN) https://youtu.be/O9Ac_vGHquM?si=oQBDke9m6FJXdrhc https://youtu.be/BPuD1l2hEQ4?si=XHzAkIC6HG-mFRBj https://youtu.be/CXU8D3kyxIc?si=B08A98ymh5vYXJ7a Tuesday - Perform basic operations with EC2 + Launch EC2 instances + Take snapshots of EC2 instances + Install applications on EC2 https://000004.awsstudygroup.com/ Wednesday - Manage resources using tags and Resource Groups + Use tags via AWS Console: create EC2 instances, add and remove tags, filter resources by tag + Use tags via AWS CLI + Use Resource Groups https://000027.awsstudygroup.com/ Thursday - Manage resources with Amazon CloudWatch + Preparation: initialize Stack in CloudFormation + Work with CloudWatch Metrics, CloudWatch Logs, and CloudWatch Alarms + Create CloudWatch Dashboard https://000008.awsstudygroup.com/ Friday - Deploy Auto Scaling Group + Create Launch Template + Create Target Group + Initialize Load Balancer + Launch Auto Scaling Group - Learn about Lightsail + Test applications on Lightsail + Use Lightsail Load Balancer, RDS, migrate to EC2 https://000045.awsstudygroup.com/ https://000006.awsstudygroup.com/ "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "Simplifying sustainability reporting using AWS and generative AI in banking by Sachin Kulkarni and Otis Antoniou on 26 JUN 2025 in Amazon Bedrock, Financial Services, Generative AI, Sustainability Permalink Share\nEuropean banks face a new challenge with the European Commission’s transition from the Non-Financial Reporting Directive (NFRD) to the Corporate Sustainability Reporting Directive (CSRD) regulations. This transition represents an expansion in sustainability reporting scope that will affect approximately 50,000 companies, a significant increase from the previous 11,700.\nThis means that banks themselves need to file sustainability reports because they will now be one of those 50,000 companies, but for their own reporting, they also need to assess their clients’ sustainability reports because they lend or finance those companies.\nIn this post, you learn how you can use generative AI services on Amazon Web Services (AWS) to automate your sustainability reporting requirements, reduce manual effort, and improve accuracy. You do this by implementing an automated solution for extracting, processing, and validating data from corporate reports.\nThe challenge Financial institutions and sustainability teams managing sustainability reporting face three critical challenges:\nScale and complexity: Banks and financial institutions must process thousands of annual reports and sustainability documents, often spanning hundreds of pages each. This process requires extensive data extraction, complex EU Taxonomy alignment calculations, and resource-intensive validation steps. Manual processing introduces significant risks of errors and consumes valuable team resources. Regulatory compliance: Banks must now implement detailed CSRD requirements, track specific metrics for turnover, capital expenditure (CapEx), and operating expenses (OpEx), and calculate their Green Asset Ratio (GAR) as well as environmental risks that come with their loans, debt, or equity investments. These new requirements demand robust data collection and processing capabilities. Data management: Processing Green House Gas (GHG) emissions data across Scope 1, 2, and 3 categories requires analyzing complex lending and investment activities. With strict reporting deadlines, organizations need efficient tools to process this expanding volume of sustainability data. The sustainability team point of view Banks finance a large variety of counterparties and economic activities. While their carbon footprint is primarily linked to the greenhouse gas (GHG) emissions of their counterparties (Scope 3), The direct GHG emissions (Scope 1) of financial institutions or the GHG emissions linked to their energy consumption (Scope 2) are usually limited. For banks, the most critical key performance indicator (KPI) is the GAR, which measures the proportion of a bank’s taxonomy-aligned balance sheet exposures versus its total eligible exposures, as shown in the following figure.\nTo calculate their GAR, banks must obtain and use sustainability data from annual reports or sustainability reports of up to 50,000 companies (many of which are subject to NFRD and CSRD reporting), and understand how much of their activities are linked to EU Taxonomy.\nThe manual process In the example that follows, we use the Amazon 2023 Annual Report. Some of the data that teams would have to manually extract includes: Revenue, Scope 1, Scope 2, and Scope 3 emissions.\nAs you can see from the page count at the top of the preceding figure, people manually searching for this data would have to go through 92 pages to find the parameters they’re looking for. Next, we might determine that some of the data we need (Scope 1, Scope 2, Scope 3) isn’t available in the annual report, so we need to analyze the sustainability report. As shown in the following figure, to manually retrieve the relevant data from this report, we would have to go through 98 pages of information.\nTo prepare a GAR, we would have to repeat this process across hundreds or even thousands of companies.\nA solution using AWS and generative AI To address these challenges, we propose an automated approach using AWS services. This approach can help banks streamline their sustainability reporting processes.\nHere’s how this solution works— as shown in the preceding figure:\nUpload your counterparties’ reports to Amazon Simple Storage Service (Amazon S3). Amazon Bedrock automatically: a. Determines NFRD eligibility. b. Extracts relevant sustainability data. c. Organizes information for GAR calculations. Review and validate the extracted data. Generate required regulatory reports. Architecture We divide the architecture into two areas:\nData ingestion flow Report generation flow Data ingestion flow\nWe use Amazon Bedrock Knowledge Bases to build an automated data ingestion flow. See Prerequisites for your Amazon Bedrock knowledge base data to understand supported document formats and limits for knowledge base data.\nThe workflow, shown in the preceding figure, is:\nAnnual reports or sustainability reports are uploaded into an S3 bucket. On the S3 bucket, we enable event notifications for events such as addition, change, or deletion of the reports. These events are sent to Amazon Event Bridge, which trigger an AWS Lambda function. The Lambda function syncs the data source to an Amazon Bedrock knowledge base. Amazon Bedrock Knowledge Bases processes the documents and converts it into vector embeddings. For more information, see Amazon Bedrock Knowledge Bases supports advanced parsing, chunking, and query reformulation giving greater control of accuracy in RAG based applications Amazon Bedrock Knowledge Bases stores the vector embeddings in the vector database of your choice, such as in an Amazon OpenSearch Serverless collection. Now the data is read, broken into chunks, converted to embeddings and stored in a vector store. You use a report generation flow to ask questions about the information in the knowledge base.\nReport generation flow To automate the report generation for sustainability teams, we created the report generation flow shown in the following figure.\nThe report generation flow includes the following steps:\nWhen user uploads an annual report, the data from the report is ingested into the knowledge base, as shown in the data ingestion flow.\nA Lambda function—Invoke Bedrock Agent—is triggered to invoke an Amazon Bedrock agent.\nThe Amazon Bedrock agent determines NFRD or CSRD applicability based on various parameters such as employee numbers and annual revenues. This agent then passes on what kind of regulation to apply to a Lambda function.\nThe Lambda function Retrieve Sustainability Metrics retrieves various parameters needed for NFRD or CSRD from the annual report. a. The function receives NFRD or CSRD applicability from the Amazon Bedrock agent. b. Based on NFRD or CSRD applicability, there are specific sustainability metrics that need to be retrieved. For NFRD, there are about 15 metrics that need to be retrieved, and for CSRD, there are about 30 metrics. c. The function iteratively sends {variable} to the Amazon Bedrock flow. For example, if the metric to be retrieved is Scope 1 emission, then the Lambda function will send variable=‘Scope 1 emission’ d. The function gets the metric value from the Amazon Bedrock flow and when the required metrics are retrieved, creates a CSV file with the details.\nAmazon Bedrock flow: a. Retrieve {variable} (for example, ‘Scope 1 emission’) from the annual report. For this, we create a prompt, as shown in the following diagram. b. Use the prompt to fetch the value from the knowledge base. - Prompt:\n```python \u0026lt;query\u0026gt; You are an intelligent agent that helps retrieve information from a knowledgebase. Please find {{variable}}. Please return only a number and not any additional text. I only need the value so you will return one word\u0026lt;/query\u0026gt; ``` c. Return the value to the Lambda function in Step 4.\nBreakdown of key components Amazon S3 is used for storing annual statements and sustainability reports, providing highly durable and secure object storage that facilitate immediate access when needed for processing.\nAmazon Bedrock Knowledge Bases enables using Retrieval-Augmented Generation (RAG) to optimize the output of a large language model by giving it the context of companies’ annual reports and regulatory requirements. It does so by creating chunks and vector embeddings from the annual reports to enable efficient information retrieval from a vector database of your choice.\nAmazon Bedrock foundation models (FMs) extract information from an Amazon Bedrock knowledge base and generate standardized PDF reports for regulators, providing consistent formatting and alignment with CSRD requirements. We encourage you to choose the best foundational model for your use case through the flexibility and enterprise-grade controls of Amazon Bedrock. For this solution, we used Anthropic’s Claude Sonnet 3.5 as the model, but by using Amazon Bedrock, you can choose from over 50 different models to see which one best fits your use case.\nAmazon Bedrock Flows orchestrates the document processing pipeline, coordinating between services to automatically extract required sustainability metrics and validate compliance requirements. This feature helps us manage the workflow from initial document ingestion through to final report generation.\nAmazon Bedrock Prompt Management creates and helps manage precise prompts that help retrieve multiple sustainability metrics from reports for example: turnover, Scope 1, Scope 2, and Scope 3 emissions data. These structured prompts facilitate consistent data extraction across different document formats.\nAmazon Bedrock Agents evaluates each uploaded document to determine NFRD or CSRD eligibility by analyzing company revenue, employee count, and incorporation details. The agents retrieve these parameters by using a Lambda function that’s part of the actions the agent can perform.\nLambda handles event-driven processing when new documents are uploaded. Lambda functions are also used by the agent to retrieve data from companies’ annual reports and trigger the appropriate workflows based on document type.\nAmazon EventBridge is used to build event-driven applications at scale across AWS and manages workflow orchestration, automatically initiating document processing when new reports are uploaded through S3 event notifications.\nThis architecture enables banks to process thousands of sustainability reports efficiently. The solution scales automatically to handle increasing document volumes while keeping security a top priority.\nAdditional considerations You can use the following additional AWS service to help further increase the accuracy of information retrieval from sustainability documents.\nAmazon Bedrock Guardrails to make sure that the solution caters to responsible AI policies. Specifically, we have added contextual grounding checks to reduce hallucinations. This is important for the solution because we’re trying to find a few specific values in a large document, and these checks make sure that the metrics retrieved are based on the documents.\nAutomated reasoning checks which help to verify the metrics returned by the solution. Consider the metric Number of employees. There can be multiple places in the annual report where the number of employees is mentioned; for example, temporary workers, part-time employees, employees from various departments, employees from a company that was taken over last year, and so on. To arrive at the right number, automated reasoning checks help.\nBenefits This sustainability reporting solution cuts document processing time from 8—10 weeks to few hours. Banks get clear audit trails showing exactly how they extracted and validated sustainability data. When regulations are updated, the system adapts through its knowledge base without disrupting operations. Built-in security protects company data through the entire process. Access controls and encryption are in place to secure information. The output delivers standardized, accurate reports. This automation lets sustainability teams concentrate on environmental improvements rather than paperwork. Teams can instead analyze trends and develop initiatives instead of hunting through reports for data points.\nConclusion As sustainability reporting requirements evolve, having a flexible and automated solution will become crucial. While we focused on NFRD reporting, the same pattern can be adapted for CSRD compliance reporting, SFDR reporting requirements, and Internal sustainability metrics, or EU Taxonomy alignment.\nCustomers looking to build their products in the Financial Services industry have access to industry and domain AWS specialists; contact us for help in your cloud journey.\nYou can also learn more about AWS services and solutions for financial services by visiting AWS for Financial Services and Generative AI on AWS.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.4-deploy-app/",
	"title": "Deploy Application",
	"tags": [],
	"description": "",
	"content": "We will use the Terminal on your local machine to SSH into the EC2 and install the application.\n1. SSH Connection Open Terminal in the directory containing your .pem key file and run the following commands (replace the DNS/IP with your actual address):\n# Set permission for key file (Linux/Mac only) chmod 400 workshop-aws-streamlit.pem # SSH Connection ssh -i \u0026#34;workshop-aws-streamlit.pem\u0026#34; ubuntu@\u0026lt;Your-Public-IPv4-DNS\u0026gt; 2. Install Environment Once connected, run the following commands to clone the code and install dependencies:\ncd ~ # Clone source code git clone https://github.com/Ntthuhoai/Workshop-AWS.git # Navigate to project directory cd Workshop-AWS/Credit_Card_Customers # Create virtual environment and install requirements python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt 3. Run Application Start the Streamlit app:\nstreamlit run app.py If successful, you will see the message You can now view your Streamlit app in your browser. along with the URLs.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/4-eventparticipated/",
	"title": "Event Participated",
	"tags": [],
	"description": "",
	"content": "During the internship, we participated in 2 events, each event we learned a lot of new knowledge from the speakers\u0026rsquo; sharing. ### Event 1\nEvent Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nTime: *8:30 on 06/09/2025\nLocation: 26th Floor, Bitexco Building, No. 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 2 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nTime: 09:00 on 18/09/2025\nLocation: 26th Floor, Bitexco Building, No. 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.4-week4/",
	"title": "Week 4",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals (from 29/09/2025 - 03/10/2025) Clearly understand storage services on AWS. Become proficient in deploying S3 and implementing a comprehensive backup strategy with AWS Backup. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about storage services on AWS + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family: Snowball, Snowball Edge, Snowmobile + Disaster recovery strategies on AWS: backup and recovery, pillow light, low capacity active-active, full capacity active-active + AWS Backup Tuesday - Practice with S3 bucket + Preparation: create an S3 bucket and upload data + Enable Static Website feature + Configure Block Public Access and public objects + Test the website, speed up static site with CloudFront + Move and copy S3 Objects across regions https://000057.awsstudygroup.com/ Wednesday - Deploy AWS Backup into the system: prepare infrastructure, initialize Backup Plan, set up Notification and test functionality - Practice VM Import/Export: prepare virtual machine, import VM to AWS, and export VM from AWS https://000013.awsstudygroup.com/ https://000014.awsstudygroup.com/ Thursday - Deploy File Storage Gateway to set up File Sharing connection with on-premise machines + Preparation: create an S3 bucket and EC2 for Storage Gateway + Use AWS Storage Gateway: create Storage Gateway, create File Shares, connect File Shares with on-premise machines https://000024.awsstudygroup.com/ Friday - Set up shared data storage for Windows infrastructure with FSx + Create practice environment, SSD Multi-AZ file system and HDD Multi-AZ file system + Create file share + Monitor and manage performance + Manage user sessions and open files + Enable user memory quotas and enable continuous file share access + Scale throughput and storage capacity https://000025.awsstudygroup.com/ "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "The future of customer service is here: From contact centers to experience hubs by Akshay Vemuganti on 24 JUN 2025 in Amazon Connect, Foundational (100), Thought Leadership Permalink Share\nDiscover how Amazon Connect transforms traditional contact centers into AI-powered experience hubs that reduce costs, increase efficiency, and deliver personalized customer experiences at scale.\nIn 2017, we launched Amazon Connect to solve the challenges we conquered in Amazon’s own customer service operations. Our mission: deliver simple, cost-effective technology that creates exceptional customer experiences at any scale.\nToday, AWS customers use Amazon Connect to support more than 10 million contact center interactions every day. But this is just the beginning. Our vision demands a fundamental shift in thinking: contact centers are no longer mere responsive hubs – they are experience centers that revolutionize how customers and businesses interact.\nThis transformation represents a seismic shift in customer engagement. Traditional contact centers wait for customer problems; experience centers anticipate and prevent them. Where contact centers focus on resolution times, experience centers create meaningful connections. This isn’t just an evolution in technology – it’s a revolution in relationship building between brands and their customers.\nImagine a world where every customer interaction is a perfectly orchestrated symphony of human empathy and AI intelligence, anticipating needs before they’re expressed. A world where geographical boundaries dissolve as AI-empowered experts delivers flawless service, making every customer feel locally served, regardless of location. Picture service environments as living organisms, constantly learning, healing, and evolving – where every interaction enhances the entire system’s intelligence. We understand customer journeys so profoundly that we craft magical moments of delight, transforming routine interactions into unforgettable experiences. Building conversational flows becomes as natural as thought itself, where human creativity and data capabilities continuously reinvent AI-powered customer experiences.\nThis is the future we’re creating with Amazon Connect.\nOur path forward: Four transformational areas reshaping customer service. 1/ The power of predicting and resolving customer intent remains core to our agentic AI vision, with our agent assistance empowering agents to deliver faster, more accurate responses. The result is a truly personalized, automated conversational self-service experience that sets new standards in customer engagement.\n2/ Our unified customer intelligence approach breaks down traditional data silos through real-time profile unification across all touchpoints. By leveraging predictive analytics to anticipate customer needs and orchestrating seamless journeys across all channels, we enable proactive engagement based on customer signals, creating a truly unified customer experience.\n3/ Global scale and reliability form the backbone of Amazon Connect through active-active deployment across multiple regions. Enhanced global telephony coverage combines with simplified capacity management to deliver unified and reliable global operations, ensuring consistent service delivery worldwide.\n4/ Agent empowerment transforms the frontline experience through AI-powered assistance for every interaction, including real-time coaching guidance, and generates deep insights from customer conversations. A unified workspace for all channels streamlines operations, while automated coaching and development features enhance agent capabilities. Simplified tools and workflows reduce complexity, allowing agents to focus on delivering exceptional customer service.\nThe paradigm shift This transformation from contact centers to experience centers fundamentally changes business-customer relationships. Experience centers become strategic assets that drive business growth, not cost centers to be optimized. They transform from problem-solving facilities into opportunity-creating engines that:\n– Generate real-time customer insights that inform product development\n– Create proactive engagement opportunities that drive sales\n– Build deeper emotional connections with customers through personalized experiences\n– Turn service interactions into brand advocacy moments\nOrganizations that embrace this shift can gain a powerful competitive advantage. They don’t just respond to customer needs – they anticipate and exceed them, creating experiences that customers actively seek out rather than interactions they merely tolerate.\nWhy this matters The customer experience reaches new heights through personalized service across all channels, faster issue resolution, and reduced customer effort. Proactive engagement anticipates and addresses customer needs before they become concerns, creating seamless interactions that build lasting relationships.\nThe agent experience sees a dramatic improvement as seen with TransUnion, a 40%+ savings in annual costs. Reduced training time and higher job satisfaction lead to better customer outcomes, creating a positive cycle of continuous improvement and excellence in service delivery.\nBusiness agility becomes a competitive advantage through rapid innovation and experimentation capabilities. Organizations can quickly adapt to market changes, leverage global operational flexibility, and implement continuous improvements to stay ahead of evolving customer expectations.\nReal results, real impact Intuit’s flexibly scales its contact center from 6,000 to 11,000 agents during tax season with pay-per-use pricing, while maintaining capacity for up to 20,000 agents. Capital One’s CIO Gill Haus emphasized the value of their tech-focused approach, stating, “Amazon Connect enables us to innovate like a tech company, meeting customer needs with speed and agility in a rapidly changing industry.” The State of Rhode Island showcased unparalleled speed by deploying their entire contact center in just 10 days, successfully handling 75,000 unemployment claims on day one.\nThe path forward As we look to 2025 and beyond, our commitment focuses on expanding the boundaries of AI in customer service, simplifying and enhancing global operations reliability, creating more personalized and proactive customer experiences, empowering agents to deliver exceptional service, and enabling rapid innovation and experimentation.\nJoin us on this journey The future of customer service is unfolding now with Amazon Connect, where contact centers evolve into experience centers that redefine customer engagement. This isn’t just about better service – it’s about creating a new paradigm for business-customer relationships. Be part of this transformation, where exceptional customer experiences become the standard and every interaction strengthens the bond between customer and brand.\nTogether, we will make every customer interaction better than the last, turning routine touchpoints into opportunities for meaningful connection and growth. The question isn’t whether to make this transformation, but how quickly you can lead it in your industry.\nReady to revolutionize your customer experience? Connect with our team today to start your transformation journey – the future of service can’t wait.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.5-test-app/",
	"title": "Test Application",
	"tags": [],
	"description": "",
	"content": "Access Web App Open a web browser on your computer.\nNavigate to: http://\u0026lt;EC2-Public-IP\u0026gt;:8501\n(Example: http://13.214.148.217:8501)\nThe credit card prediction app interface will appear. Try entering parameters and clicking the predict button.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.5-week5/",
	"title": "Week 5",
	"tags": [],
	"description": "",
	"content": "Week 5 Goals (from 06/10/2025 - 10/10/2025) Understand security services on AWS. Master IAM (Roles, Policies) principles to manage secure access. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about security services on AWS + Shared Responsibility Model + AWS Identity and Access Management (IAM) + Amazon Cognito Tuesday - Implement access control with IAM: + Create IAM Group and IAM User: create Admin Group, Admin User, log in with Admin User account + Create IAM Role and IAM User: create Admin Role, Operator User + Switch IAM Role https://000002.awsstudygroup.com/ Wednesday - Review IAM theory - Practice IAM Role and Condition + Create EC2, RDS admin users + Create management groups + Configure IAM Role Condition: create an admin IAM Role, restrict role access by configuring switch role based on IP and time limits https://000044.awsstudygroup.com/ Thursday - Grant application access to AWS services with IAM roles + Create EC2 instance and S3 bucket + Create IAM User and Access Key, use the Access Key + Create and use IAM role on EC2 https://000048.awsstudygroup.com/ Friday - Limit user permissions with IAM Permission Boundary + Create restricted Policy + Create IAM user with limited permissions + Test restricted user https://000030.awsstudygroup.com/ "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/",
	"title": "WorkShop",
	"tags": [],
	"description": "",
	"content": "Deploy Machine Learning Model with Streamlit on EC2 Overview In this workshop, you will learn how to deploy a Machine Learning application (Credit Card Customer prediction) using the Streamlit framework on Amazon EC2 infrastructure.\nInstead of using complex managed services, we will optimize costs and processes by hosting the entire application (Front-end and Model) directly on a single Ubuntu EC2 instance.\nArchitecture Content Workshop Overview Prerequisites Launch EC2 Instance Deploy Application Test Application Clean up "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "Optimize HPC Workloads with the Updated AWS Well-Architected Lens by Max Starr and Marissa Powers on 24 JUN 2025 in AWS Well-Architected Framework, High Performance Computing Permalink Share\nThis post was contributed by Max Starr PhD and Marissa E Powers PhD\nThe AWS Well-Architected Framework helps you address critical challenges in building cloud architectures by providing proven architectural best practices. High Performance Computing (HPC) environments require specialized architectural considerations that differ from typical cloud workloads. As organizations migrate their HPC workloads to AWS or build new HPC capabilities in the cloud, they often face questions about architectural best practices:\nHow do you ensure your HPC architecture follows security and cost optimization best practices? What are the recommended patterns for scaling compute resources efficiently? How do you optimize storage and networking for HPC-specific requirements? Which AWS services best suit different types of HPC workloads? Until now, getting answers to these questions typically required direct consultation with AWS Solutions Architects who specialize in HPC. Today, we’re announcing an update to the AWS Well-Architected High Performance Computing (HPC) Lens whitepaper, providing you with comprehensive architectural guidance in a single, authoritative resource. This newly updated lens consolidates years of AWS HPC implementation experience into structured best practices that help you build and optimize your HPC workloads on AWS.\nThe HPC Lens is part of the AWS Well-Architected Framework, which has been helping customers improve their cloud architectures since 2015. The Framework addresses six key pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. The HPC Lens extends this guidance to address the unique challenges of HPC workloads.\nWhat’s new in the HPC Lens? Since the last HPC Lens release in 2019, AWS introduced numerous advancements in HPC capabilities. The updated lens incorporates these new technologies and learnings, providing you with current best practices for HPC workloads. Key updates include:\nAWS ParallelCluster advancements for building and managing HPC clusters New HPC-optimized compute instance families designed for specific workload types Enhanced storage options and best practices for high-performance file systems Advanced networking capabilities for improved application performance Integration guidance for managed services including AWS Batch, Amazon FSx and AWS Parallel Computing Service Updated security and compliance recommendations New cost optimization strategies for HPC workloads These updates provide you with comprehensive guidance to design, deploy, and optimize your HPC workloads on AWS, incorporating the latest technological advancements and learned best practices from real-world implementations.\nWho should use the updated Lens? The HPC Lens is particularly valuable if you are:\nPlanning a new HPC deployment on AWS Evaluating your existing HPC architecture against current best practices Optimizing performance, security, or costs of your HPC workloads Preparing for an increase in scale or complexity of your HPC environment Whether you’re a seasoned HPC professional or new to cloud-based HPC, the lens provides valuable insights to enhance your architecture.\nGet started To begin building a well-architected HPC environment on AWS:\nReview the AWS Well-Architected Framework fundamentals. Read the AWS Well-Architected HPC Lens whitepaper. Review your workload using the AWS Well-Architected Tool, incorporating both Framework and HPC Lens questions. To learn more about AWS HPC capabilities, visit the AWS HPC Solutions page.\nConclusion The updated AWS Well-Architected HPC Lens provides you with a comprehensive framework to design, deploy, and optimize your HPC workloads on AWS. By incorporating the latest advancements in HPC technologies and best practices, this lens helps you build efficient, secure, and cost-effective HPC environments. Whether you’re new to HPC on AWS or looking to enhance your existing architecture, the HPC Lens offers valuable insights to drive your projects forward. We encourage you to download the whitepaper, explore the resources provided, and start applying these best practices to your HPC workloads today. As always, we welcome your feedback and look forward to seeing the innovative HPC solutions you build on AWS.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "To avoid incurring costs, clean up resources after finishing the workshop:\n1. Terminate EC2 Go to EC2 Dashboard \u0026gt; Instances. Select instance workshop-aws-streamlit. Choose Instance State \u0026gt; Terminate instance. 2. Delete Security Group (Optional) Delete the Security Group if no longer needed.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/6-self-evaluation/",
	"title": "Self-evaluation",
	"tags": [],
	"description": "",
	"content": "Throughout my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09/2025 to 22/11/2025, I had the opportunity to experience the work environment, practice, and apply the knowledge acquired at school to a practical project. At the same time, I also learned much new knowledge through various learning modules.\nRegarding my work conduct, I always strived to complete tasks well and comply with the established regulations.\nTo objectively reflect on the internship process, I would like to self-assess based on the following criteria:\nNo. Criteria Description Excellent Good Average 1 Professional Knowledge and Skills Industry understanding, practical application of knowledge, tool usage skills, work quality ✅ ☐ ☐ 2 Ability to Learn Absorbing new knowledge, fast learning ☐ ✅ ☐ 3 Proactiveness Self-research, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of Responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to working hours, regulations, and work procedures ☐ ✅ ☐ 6 Growth Mindset (Aspiration for Improvement) Willingness to receive feedback and self-improve ✅ ☐ ☐ 7 Communication Presenting ideas, reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues, participating in the team ☐ ✅ ☐ 9 Professional Conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-Solving Thinking Identifying issues, proposing solutions, creativity ☐ ✅ ☐ 11 Contribution to the Project/Organization Work efficiency, improvement initiatives, recognition from the team ☐ ✅ ☐ 12 Overall Assessment General evaluation of the entire internship process ✅ ☐ ☐ Areas for Improvement Improving my problem-solving thinking Being more open-minded, learning daily and work communication skills, and learning how to handle situations "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.6-week6/",
	"title": "Week 6",
	"tags": [],
	"description": "",
	"content": "Week 6 Goals (from 13/10/2025 - 17/10/2025) Continue exploring security services on AWS. Understand how AWS Security Hub works. Learn about static and dynamic encryption and how to manage keys with KMS. Review Module 5 content. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about security services on AWS + AWS Organization \u0026amp; AWS Identity Center + AWS KMS + AWS Security Hub Tuesday - Revisit the practice exercise on managing access to EC2 services using Tags and Resource Groups via IAM (from Week 3): + Using Tags: use tags via console, display, add or remove tags, assign tags to a virtual machine, filter resources by tags, and use tags via CLI + Use Resource Groups https://0000027.awsstudygroup.com/ Wednesday - Read documentation and practice data encryption with KMS + Set up environment: create policy, role, group, and user + Create KMS key management service + Create Amazon S3 + Encrypt with AWS KMS + Create AWS CloudTrail and Amazon Athena + Test and share encrypted data on S3 https://000033.awsstudygroup.com/ Thursday - Use AWS Security Hub + Read about security standards + Activate Security Hub + Test and evaluate each security standard https://000018.awsstudygroup.com/ Friday - Review theoretical and practical content of Module 5 "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Intern Feedback Report General Assessment 1. Working Environment The working environment is highly professional and friendly. All colleagues and FCJ members are always willing to provide support, even outside working hours. The workspace is tidy, comfortable, and well-equipped, ensuring a smooth internship experience.\n2. Support from Mentor / Team Admin My Mentor provided very detailed guidance, clearly explained anything I did not understand, and always encouraged me to ask questions. The Team Admin supported all procedures, documents, and helped create a favorable working environment for me.\n3. Alignment between Work and Academic Major The tasks I was assigned aligned well with the knowledge I learned at university, while also exposing me to new areas I had not encountered before. This allowed me to strengthen my foundational understanding and develop practical skills.\n4. Opportunity for Learning \u0026amp; Skill Development During the internship, I gained a lot of new knowledge about the Cloud domain, as well as skills such as teamwork and communication within a corporate environment. The Mentor also shared valuable real-world experience, helping me better orient my career path.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: people are respectful, work seriously yet cheerfully. Everyone collaborates and supports each other regardless of their position.\n6. Policies / Benefits for Interns The company offered flexible working hours, which was very helpful.\nOther Questions What were you most satisfied with during the internship? The dedicated support and assistance from the senior colleagues.\nWhat do you think the company needs to improve for future interns? After 12 weeks of internship, I find everything to be good, and I currently have no suggestions.\nIf recommending to friends, would you advise them to intern here? Why? Yes. They would have the opportunity to work with a professional team and gain valuable experience to support their career development.\nProposals \u0026amp; Wishes Do you want this program to continue in the future? I hope the program will continue in the future to inspire and share knowledge with young people like us.\nOther feedback (free sharing): We sincerely thank the senior colleagues for creating the conditions and providing support, for always being ready to answer our questions, and for being understanding of our shortcomings throughout this period.\n"
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.7-week7/",
	"title": "Week 7",
	"tags": [],
	"description": "",
	"content": "Week 7 Goals (from 20/10/2025 - 24/10/2025) Understand database services on AWS. Learn how to create databases with Amazon RDS and connect applications to databases. Learn how to perform schema conversion and migrate data. Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about databases and key concepts + Session + Primary Key, Foreign Key + Database Index + Partitioning, Query Execution Plan, Query Plan + Database Log, Buffer + RDBMS, NoSQL + OLTP and OLAP Tuesday - Learn about database services on AWS + Amazon RDS + Amazon Aurora + Amazon ElastiCache + Amazon RedShift Wednesday - Practice with Amazon Relational Database (RDS) + Set up network infrastructure and necessary security components: VPC, subnet, Security Group for EC2, Security Group for RDS DB, and DB Subnet for Amazon RDS + Deploy application + Backup and restore https://000005.awsstudygroup.com/ Thursday - Practice schema conversion and data migration with AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) https://000043.awsstudygroup.com/ Friday - Review theoretical and practical content of Module 6 "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.8-week8/",
	"title": "Week 8",
	"tags": [],
	"description": "",
	"content": "Week 8 Goals (from 27/10/2025 - 31/10/2025) Practice data analysis on AWS Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about the concept and characteristics of Data Lake - Perform data exploration, analysis, and statistics + Create IAM Role for AWS Glue, create policy + Collect and store data: create S3 Bucket, create distribution flow, create sample data + Create a data catalog: create Glue Crawler and check the data https://000035.awsstudygroup.com/ Tuesday - Continue data practice + Perform data transformation with two methods of creating SageMaker Notebook: AWS Studio and AWS Management Console, run and understand the code in the practice section https://000035.awsstudygroup.com/ Wednesday - Learn about Amazon Athena and Amazon QuickSight - Perform data queries with Amazon Athena and visualize data with Amazon QuickSight https://000035.awsstudygroup.com/ Thursday - Analyze costs and performance with AWS Glue and Amazon Athena - Preparation steps + Prepare database: use existing data from AWS Cost \u0026amp; Usage Report, create S3 bucket, create storage folder and upload data files + Build database: configure Amazon Athena to access data files via AWS Glue + Test database - Analyze costs and system performance using SQL queries with Amazon Athena + Study the data in the table + Study costs + Optimize cost allocation and measure job efficiency with tags + View and compare cost results when using EC2 with Savings Plans or Reserved Instances vs. On Demand https://000040.awsstudygroup.com/ Friday - Review the theoretical and practical content completed during the week "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.9-week9/",
	"title": "Week 9",
	"tags": [],
	"description": "",
	"content": "Week 9 Goals (from 03/11/2025 - 07/11/2025) Continue practicing data analysis on AWS Tasks to be carried out this week: Day Tasks Reference Materials Monday - Learn about the NoSQL database service Amazon DynamoDB + Core components of Amazon DynamoDB: item, table, attribute + Primary Key: partition key, composite primary key + Secondary Index + Naming rules and data types + Read Consistency + Read/Write Capacity Mode - Use both AWS Management Console and AWS CloudShell to perform preparation steps such as creating tables, writing-reading-updating-querying data, creating and querying Global Secondary Index - Practice with AWS SDK + Configure AWS CLI + Use AWS SDK for Python to write programs performing operations on Amazon DynamoDB https://000060.awsstudygroup.com/ Tuesday - Conduct the Analytics workshop following instructions - Preparation: create S3 bucket - Ingest and store data: + Create Amazon Kinesis Data Firehose + Create dummy data - Catalog Data: create IAM Role, AWS Glue Crawlers, and verify the tables created in the catalog - Transform data with Glue (interactive sessions) - Transform data with AWS Glue DataBrew - Transform data with EMR https://000072.awsstudygroup.com/ Wednesday Continue the Analytics workshop - Analyze data with Athena - Analyze data with Kinesis Data Analytics - Visualize data in QuickSight - Set up an Amazon Redshift cluster and use AWS Glue to load data into Amazon Redshift https://000072.awsstudygroup.com/ Thursday - Build dashboards to visualize data with Amazon QuickSight + Sign up for QuickSight + Build dashboards: update datasets, create line charts, KPIs and Insights, pie charts, create pivot tables, and complete the dashboard + Improve the dashboard by formatting, adding charts and detailed data tables https://000073.awsstudygroup.com/ Friday - Review all theoretical and practical content of Module 7 "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.10-week10/",
	"title": "Week 10",
	"tags": [],
	"description": "",
	"content": "Week 10 (from 10/11/2025 - 14/11/2025) Propose a Workshop topic Write a proposal and outline the basic architecture "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.11-week11/",
	"title": "Week 11",
	"tags": [],
	"description": "",
	"content": "Week 11 (from 17/11/2025 - 22/11/2025) Complete the proposal and architecture Finalize the Hugo template and deploy it to GitHub "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/1-worklog/1.12-week12/",
	"title": "Week 12",
	"tags": [],
	"description": "",
	"content": "Week 12 (from 24/11/2025 - 28/11/2025) Continue implementing and optimizing the Workshop content "
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ntthuhoai.github.io/workshop.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]